- Web Crawling 이란?
 : 인터넷에 있는 정보 중 우리가 원하는 것만 골라서 자동으로 수집해주는 기술

- Why ?
 : 수 많은 자료를 검색하고 다운받으려면 많은 시간 소요 !
 : Crawling으로 데이터 수집 시간 단축 가능 !

- 어디서 활용 ?
 : 기업의 고객 정보, 마케팅 정보, 금융 데이터, 국내 지리 정보 등 수 많은 양의 데이터 존재
 1. 제품 및 서비스에 대한 고객들의 평점, 리뷰들을 Crawling 후 분석하여 고객 만족도 & 개선점 파악 !
 2. SNS 및 커뮤니티에 존재하는 Data Crawling 후 분석하여 트렌드 파악 !

- 절차 ?
 1. 정보를 가져오려는 사이트 불러오기
 2. 원하는 정보 찾기
 3. 원하는 정보 출력

- 사용하는 라이브러리 ?
 1. requests
  : python으로 HTTP 호출하는 프로그램을 작성 할 때 사용 (서버 ↔ 클라이언트 데이터 주고 받기)
  : Crawling에서 주로 get함수를 통해 웹페이지의 html 코드 전체를 불러옴.

 2. BeautifulSoup
  : 문자열을 실제 html 코드로 변환하고, html 효율적 탐색 후 원하는 정보 추출

- Crawling한 데이터 저장 방법 및 젎차 ?
 1. 파일 열기 : open("FILE NAME","auth")
 2. 파일 작성 : write(DATA)
 3. 파일 닫기 : close()

 예시) 
    file = open("newfile.txt","w")
    for i in range(1,11):
        file.write(data)
    file.close()
